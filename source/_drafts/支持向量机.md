---
title: 支持向量机
urlname: support-vector-machine
toc: true
mathjax: true
date: 2018-06-06 19:00:34
tags: MachineLearning
---

刚才勉强看完了SVM的讲义，发现其中的实际内容梳理起来并不多，也就以下几点：
* 线性可分情况下的SVM
  * 硬间隔的定义
  * 原问题和对偶问题
* 线性不可分情况下的SVM
  * 软间隔的定义
  * 引入松弛变量后的原问题和对偶问题
* 核函数
* 核函数支持向量机

但是要求的数学有点儿多……

先讲讲SVM到底是个啥好了。实际上，只是需要对某个空间内的两类点，找到一个超平面把它们分割开来。如果在这个空间内做不到，那么还可以运用核函数，变换到特征空间，总能找到一个合适的超平面。如何找到合适的平面呢？显然，对于明显可以区分开来的两类点，平面取决于那些在**边缘**的点。这些点就是所谓的“支持向量”。

## 推导SVM的基本型

这一部分的思路主要来自[一篇非常好的知乎问答](https://www.zhihu.com/question/21094489/answer/117246987)。当我在阅读周志华的书的时候，我非常困惑于两个附加的超平面是怎么来的，而我在这里找到了答案。

所谓“SVM的基本型”指的是下列凸优化问题：

$$ \underset{\mathbf{w}, b}{\mathrm{argmin}} \frac{1}{2} \Vert \mathbf{w} \Vert^2$$
$$ s.t. \space y_i (\langle \mathbf{w}, \mathbf{x}_i\rangle + b) \geq 1$$

下面我尝试用（至少在我看来）简单易懂的方法来推导这一基本型。

我们考虑二维的情况。对于线性可分的情况，确实存在一条直线能够把两种点分开。此时直线两边到直线距离最短的点称为支持向量（显然，可能有不止一个）。现在，我们的目标就是，找到这样一条直线，使得离它最近的点的距离最大。由于两侧都有点，因此，在该直线的斜率不变的情况下，平移它使得直线到两侧的支持向量的距离相等是最优的。此时，我们定义**间隔**（margin；此处定义的实际上是硬间隔）为一侧的支持向量到直线的距离。可以“形式化”地把问题表达成这样：

$$\underset{line}{\mathrm{argmax}} \{margin\}$$

下面开始尝试用真·形式化的方法表达这一问题。并不难，大概都是高中计算几何讲过的。

空间中的平面可以通过下列线性方程来描述：

$$ \langle \mathbf{w}, \mathbf{x} \rangle + b = 0 $$

其中$\mathbf{w}$是垂直于平面的法向量，b是位移，决定了平面到原点的距离。

任意点$\mathbf{x}_0$到该平面的距离可以用下式表示：

$$ \frac{1}{\Vert \mathbf{w} \Vert} (\langle \mathbf{w}, \mathbf{x}_0 \rangle + b)$$

显然这个距离是可以为负的。那么不妨假设分类为$+1$的点在距离为正的一侧，分类为$-1$的点在距离为负的一侧，把距离乘上点的分类，就可以得到一个必然为正的距离。于是我们得到了**间隔**的数学表达式：

$$\gamma_i = \min_{i} \frac{1}{\Vert \mathbf{w} \Vert} y_i(\langle \mathbf{w}, \mathbf{x}_i \rangle + b)$$

<!--代入到原问题中，得到下式：

$$\underset{\mathbf{w}, b}{\mathrm{argmax}} \left\{ \frac{1}{\Vert \mathbf{w} \Vert} \min_{i} y_i(\mathbf{w}^T \mathbf{x}_i + b) \right\}$$-->

我们可以把这个问题换一种表达形式。如果我们不把间隔的表达式直接代入，而是拿出来作为一个变量，就可以得到下面的问题和约束条件：

$$ \underset{\mathbf{w}, b}{\mathrm{argmax}} \{margin\} $$

$$ s.t. \space \frac{1}{\Vert \mathbf{w} \Vert} y_i(\langle \mathbf{w}, \mathbf{x}_i \rangle + b) \geq margin, \forall i $$

<del>原作者似乎在此处进行了一些信仰之跃</del>

<details>
<summary>点击查看信仰之跃（简单的等价变换)</summary>

总之一个长上面那样子的问题是没有办法进行优化的。[这份讲义](http://www.cs.tufts.edu/~roni/Teaching/CLT2008S/LN/lecture21-22.pdf)提供了一个不错的化简思路：

由于$\mathbf{w}$是法向量，因此不妨设$\Vert \mathbf{w} \Vert = 1$。这样，原问题就可以写成以下形式：

$$\underset{\mathbf{w}, b}{\mathrm{argmin}} \min_{i} \gamma_i$$

令$\gamma$表示训练集的间隔，则显然，训练集中每个样例点到超平面的距离都不超过$\gamma$，原问题可以写成以下形式：

$$\underset{\mathbf{w}, b}{\mathrm{argmin}} \gamma$$
$$ \forall i,  y_i(\langle \mathbf{w}, \mathbf{x}_i \rangle + b) \geq \gamma$$

对上式进行一些变换：

$$ y_i((\frac{\mathbf{w}}{\gamma})^T \mathbf{x}_i + \frac{b}{\gamma}) \geq 1$$

记$\mathbf{w}' = \frac{\mathbf{w}}{\gamma}$，$b' = \frac{b}{\gamma}$，此时原式变为：

$$ y_i((\langle \mathbf{w}', \mathbf{x}_i \rangle + b') \geq 1$$

由于$\Vert \mathbf{w} \Vert = 1$，我们可以计算出$\mathbf{w}'$和$\gamma$的关系：

$$\Vert \mathbf{w}' \Vert^2 = \Vert \frac{\mathbf{w}}{\gamma} \Vert^2 = \frac{1}{\gamma^2} \Vert \mathbf{w} \Vert^2 = \frac{1}{\gamma^2}$$

因此，最小化$\gamma$也就相当于最大化$\frac{1}{\gamma^2}$，即最大化$\Vert \mathbf{w}' \Vert^2$。为了更符合凸优化（大概？）问题的形式，把优化目标修改成$\frac{1}{2} \Vert \mathbf{w}' \Vert^2$，于是此时问题变成了：

$$\underset{\mathbf{w}, b}{\mathrm{argmin}} \frac{1}{2} \Vert \mathbf{w}' \Vert^2 $$
$$ y_i((\langle \mathbf{w}', \mathbf{x}_i \rangle + b') \geq 1$$

把$\mathbf{w}'$和$b'$换成$\mathbf{w}$和$b$，就可以得到SVM的基本型了。

</details>

将上述问题进行一些简单的等价变换，就可以得到SVM的基本型了：

$$ \underset{\mathbf{w}, b}{\mathrm{argmin}} \frac{1}{2} \Vert \mathbf{w} \Vert^2$$
$$ s.t. \space y_i (\langle \mathbf{w}, \mathbf{x}_i \rangle + b) \geq 1$$

## 用Lagrange乘子法得到对偶问题

说实话，我现在还没有理解Lagrange乘子法的精髓。所以先不管。总之，我们可以通过在原问题里添加乘子得到下式：

$$ L(\mathbf{w}, b, \mathbf{\alpha}) = \frac{1}{2} \Vert \mathbf{w} \Vert^2 - \sum_i \alpha_i[y_i (\langle \mathbf{w}, \mathbf{x}_i \rangle + b) - 1]$$

我们希望能够在L最小化的同时做到乘子的最大化。为了使L能够取到最小值，显然需要$\frac{\partial L}{\partial \mathbf{w}} = \frac{\partial L}{\partial b} = 0$，因此：

$$\frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_i \alpha_i y_i \mathbf{x}_i = 0$$

$$\frac{\partial L}{\partial b} = \sum_i \alpha_i y_i = 0$$

将这两个式子代回到$L$的表达式中，发现恰好消掉了$b$，于是得到：

$$L(\mathbf{\alpha}) = \sum_i \alpha_i - \frac{1}{2} \sum_i \sum_j \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle$$
$$s.t. \space \sum_i \alpha_i y_i = 0, \alpha_i \geq 0$$

这就是SVM的对偶问题。

……支持向量……

在求解对偶问题时，我们需要求解的是使$L(\mathbf{\alpha})$最大的$\mathbf{\alpha}$的取值，这一点可以通过各种手段做到（SMO算法或者直接求导）。假设我们已经求出了对应的$\mathbf{\alpha}$的取值，这时可以立即计算出$\mathbf{w}$：

$$\mathbf{w} = \sum_i \alpha_i y_i \mathbf{x}_i$$

## 一个线性SVM求解的具体例子

## 线性不可分情况下的SVM

### 一个线性不可分情况下的SVM例子

## 核函数

## 核函数SVM



<details>
<summary>This is what you want to show before expanding</summary>
<p>This is where you put the details that are shown once expanded</p>
**a**
$$a + b = c$$
</details>
